#!/usr/bin/env python3
"""
F1 Experiment Data Processing Test

Test script for F1 experiments with dual-ball learning paradigm.
Based on the ballpushing_metrics.py test framework but adapted for F1-specific metrics.

Usage:
    python f1_metrics_test.py --mode fly --path /path/to/f1/fly --metrics all
    python f1_metrics_test.py --mode experiment --path /path/to/f1/experiment --metrics f1_specific
"""

import argparse
from pathlib import Path
import json
import numpy as np
import pandas as pd
import time
import sys
import traceback
from datetime import datetime

# Add the src directory to the path for imports
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from Ballpushing_utils import Fly, Experiment, BallPushingMetrics, F1Metrics


def convert_to_serializable(obj):
    """Convert numpy types to JSON serializable types."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (list, tuple)):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    else:
        return obj


def get_f1_metric_sets():
    """Get predefined sets of F1-specific metrics for testing."""
    return {
        "f1_basic": [
            "adjusted_time",
            "direction_match",
        ],
        "f1_checkpoints": [
            "F1_checkpoints",
        ],
        "f1_ball_distances": [
            "training_ball_distances",
            "test_ball_distances",
        ],
        "f1_comprehensive": [
            "adjusted_time",
            "training_ball_distances",
            "test_ball_distances",
            "F1_checkpoints",
            "direction_match",
        ],
        "ballpushing_basic": [
            "nb_events",
            "max_event",
            "max_event_time",
            "final_event",
            "final_event_time",
            "chamber_time",
            "chamber_ratio",
        ],
        "ballpushing_comprehensive": [
            "nb_events",
            "max_event",
            "final_event",
            "has_finished",
            "has_major",
            "has_significant",
            "chamber_ratio",
            "distance_moved",
            "pulling_ratio",
        ],
        "combined_analysis": [
            # F1-specific
            "adjusted_time",
            "F1_checkpoints",
            "direction_match",
            # Standard ballpushing
            "nb_events",
            "max_event",
            "final_event",
            "chamber_ratio",
            "has_finished",
        ],
    }


def validate_f1_fly_directory(fly_path):
    """
    Validate that the F1 fly directory contains the expected files.

    Returns:
        dict: Validation results with success status and details
    """
    fly_path = Path(fly_path)
    validation = {
        "valid": True,
        "issues": [],
        "files_found": {},
        "expected_files": ["*.slp", "*ball*.slp", "*fly*.slp", "*.mp4"],
    }

    print(f"ğŸ” Validating F1 fly directory: {fly_path}")

    # Check if directory exists
    if not fly_path.exists():
        validation["valid"] = False
        validation["issues"].append(f"Directory does not exist: {fly_path}")
        return validation

    if not fly_path.is_dir():
        validation["valid"] = False
        validation["issues"].append(f"Path is not a directory: {fly_path}")
        return validation

    # Check for required files
    all_files = list(fly_path.glob("*"))
    validation["files_found"]["all_files"] = [f.name for f in all_files]

    # Check for SLEAP tracking files
    ball_files = list(fly_path.glob("*ball*.slp"))
    fly_files = list(fly_path.glob("*fly*.slp"))
    video_files = list(fly_path.glob("*.mp4"))

    validation["files_found"]["ball_tracking"] = [f.name for f in ball_files]
    validation["files_found"]["fly_tracking"] = [f.name for f in fly_files]
    validation["files_found"]["videos"] = [f.name for f in video_files]

    # Validate required files
    if not ball_files:
        validation["valid"] = False
        validation["issues"].append("No ball tracking files (*ball*.slp) found")

    if not fly_files:
        validation["valid"] = False
        validation["issues"].append("No fly tracking files (*fly*.slp) found")

    if not video_files:
        validation["issues"].append("No video files (*.mp4) found - may not be critical")

    # Check for expected F1 structure (single or dual balls are both valid)
    if len(ball_files) == 0:
        validation["valid"] = False
        validation["issues"].append("No ball tracking files found - F1 experiments require at least one ball")
    elif len(ball_files) > 2:
        validation["issues"].append(f"Multiple ball tracking files found ({len(ball_files)}) - may need investigation")
    # Note: Both single-ball and dual-ball F1 experiments are valid

    # Print validation results
    if validation["valid"]:
        print(f"âœ… F1 directory validation passed")
        print(f"   Ball tracking files: {len(ball_files)}")
        print(f"   Fly tracking files: {len(fly_files)}")
        print(f"   Video files: {len(video_files)}")
    else:
        print(f"âŒ F1 directory validation failed:")
        for issue in validation["issues"]:
            print(f"   - {issue}")

    if validation["issues"]:
        print(f"âš ï¸  Validation warnings:")
        for issue in validation["issues"]:
            print(f"   - {issue}")

    return validation


def test_f1_fly_initialization(fly_path):
    """
    Test F1 fly initialization and basic properties.

    Returns:
        dict: Test results
    """
    print(f"\nğŸ§ª Testing F1 Fly Initialization")
    print(f"{'='*60}")

    results = {
        "initialization": {"success": False, "error": None},
        "tracking_data": {"success": False, "details": {}},
        "f1_compatibility": {"success": False, "details": {}},
    }

    try:
        # Initialize F1 fly
        print(f"ğŸ“ Loading F1 fly from: {fly_path}")
        start_time = time.time()

        f1_fly = Fly(fly_path, as_individual=True)

        load_time = time.time() - start_time
        print(f"âœ… Fly loaded successfully in {load_time:.3f}s")

        results["initialization"]["success"] = True
        results["initialization"]["load_time"] = load_time

        # Test tracking data
        tracking_data = f1_fly.tracking_data

        if tracking_data is None:
            print(f"âŒ Tracking data is None - file loading failed")
            results["tracking_data"]["success"] = False
            results["tracking_data"]["error"] = "tracking_data is None"

            # Try to diagnose the issue
            print(f"ğŸ” Diagnosing file loading issue...")
            print(f"   Fly directory: {fly_path}")
            print(f"   Expected fly name: {getattr(f1_fly, 'name', 'UNKNOWN')}")

            # List actual files in directory
            actual_files = list(Path(fly_path).glob("*.slp"))
            print(f"   Actual .slp files found: {[f.name for f in actual_files]}")

            # Check what files the Fly class expected
            fly_name = getattr(f1_fly, "name", "UNKNOWN")
            expected_ball_file = f"{fly_name}_tracked_ball.slp"
            expected_fly_file = f"{fly_name}_tracked_fly.slp"
            print(f"   Expected ball file: {expected_ball_file}")
            print(f"   Expected fly file: {expected_fly_file}")

            # Check if files exist with correct names
            ball_files = list(Path(fly_path).glob("*ball*.slp"))
            fly_files = list(Path(fly_path).glob("*fly*.slp"))
            print(f"   Ball files found: {[f.name for f in ball_files]}")
            print(f"   Fly files found: {[f.name for f in fly_files]}")

        elif not hasattr(tracking_data, "valid_data"):
            print(f"âŒ Tracking data object has no 'valid_data' attribute")
            results["tracking_data"]["success"] = False
            results["tracking_data"]["error"] = "tracking_data missing valid_data attribute"

        elif not tracking_data.valid_data:
            print(f"âŒ Tracking data is invalid")
            results["tracking_data"]["success"] = False
            results["tracking_data"]["error"] = "tracking_data.valid_data is False"

        else:
            print(f"âœ… Tracking data is valid")
            results["tracking_data"]["success"] = True

            # Analyze tracking data structure
            details = results["tracking_data"]["details"]
            details["duration"] = tracking_data.duration if hasattr(tracking_data, "duration") else None
            details["start_position"] = (tracking_data.start_x, tracking_data.start_y)
            details["exit_time"] = tracking_data.exit_time

            # Ball tracking analysis
            if tracking_data.balltrack and tracking_data.balltrack.objects:
                details["num_balls"] = len(tracking_data.balltrack.objects)
                details["ball_info"] = []

                for i, ball_obj in enumerate(tracking_data.balltrack.objects):
                    ball_data = ball_obj.dataset
                    ball_info = {
                        "ball_index": i,
                        "data_length": len(ball_data),
                        "initial_position": (ball_data["x_centre"].iloc[0], ball_data["y_centre"].iloc[0]),
                        "has_euclidean_distance": "euclidean_distance" in ball_data.columns,
                    }
                    details["ball_info"].append(ball_info)

                print(f"ğŸ€ Ball tracking: {details['num_balls']} balls detected")
                for ball_info in details["ball_info"]:
                    print(
                        f"   Ball {ball_info['ball_index']}: {ball_info['data_length']} frames, "
                        f"pos=({ball_info['initial_position'][0]:.1f}, {ball_info['initial_position'][1]:.1f})"
                    )
            else:
                details["num_balls"] = 0
                print(f"âŒ No ball tracking data found")

            # Fly tracking analysis
            if tracking_data.flytrack and tracking_data.flytrack.objects:
                details["num_flies"] = len(tracking_data.flytrack.objects)
                fly_data = tracking_data.flytrack.objects[0].dataset
                details["fly_data_length"] = len(fly_data)
                print(f"ğŸ› Fly tracking: {details['num_flies']} flies, {details['fly_data_length']} frames")
            else:
                details["num_flies"] = 0
                print(f"âŒ No fly tracking data found")

        # Test F1 compatibility
        if results["tracking_data"]["success"]:
            f1_details = results["f1_compatibility"]["details"]

            # Check ball setup for F1 (both single and dual ball are valid)
            num_balls = results["tracking_data"]["details"]["num_balls"]
            f1_details["num_balls"] = num_balls

            if num_balls >= 2:
                f1_details["ball_setup_type"] = "dual_ball"
                print(f"âœ… F1 dual-ball setup detected ({num_balls} balls)")

                # Check ball positioning for F1 (training vs test ball)
                ball_positions = [info["initial_position"] for info in results["tracking_data"]["details"]["ball_info"]]
                start_x = results["tracking_data"]["details"]["start_position"][0]

                distances_from_fly = [abs(pos[0] - start_x) for pos in ball_positions]
                f1_details["ball_distances_from_start"] = distances_from_fly

                # Classify balls as training vs test based on distance
                close_balls = sum(1 for d in distances_from_fly if d < 100)
                far_balls = sum(1 for d in distances_from_fly if d >= 100)

                f1_details["potential_training_balls"] = close_balls
                f1_details["potential_test_balls"] = far_balls

                print(f"ğŸ“Š Ball analysis: {close_balls} training candidates, {far_balls} test candidates")

                if close_balls >= 1 and far_balls >= 1:
                    f1_details["f1_layout_detected"] = "dual_ball_training_test"
                    results["f1_compatibility"]["success"] = True
                    print(f"âœ… F1 dual-ball training/test layout detected")
                else:
                    f1_details["f1_layout_detected"] = "dual_ball_unclear"
                    results["f1_compatibility"]["success"] = True  # Still valid F1
                    print(f"âœ… F1 dual-ball experiment (layout unclear but valid)")

            elif num_balls == 1:
                f1_details["ball_setup_type"] = "single_ball"
                f1_details["f1_layout_detected"] = "single_ball"
                results["f1_compatibility"]["success"] = True
                print(f"âœ… F1 single-ball setup detected - valid F1 experiment")

                # For single ball, just record its position
                if results["tracking_data"]["details"]["ball_info"]:
                    ball_pos = results["tracking_data"]["details"]["ball_info"][0]["initial_position"]
                    start_x = results["tracking_data"]["details"]["start_position"][0]
                    distance_from_fly = abs(ball_pos[0] - start_x)
                    f1_details["ball_distance_from_start"] = distance_from_fly
                    print(f"ğŸ“Š Ball distance from fly start: {distance_from_fly:.1f} pixels")
            else:
                f1_details["ball_setup_type"] = "no_balls"
                f1_details["f1_layout_detected"] = "invalid"
                print(f"âŒ No balls detected - invalid F1 experiment")

        return results

    except Exception as e:
        print(f"âŒ Error during F1 fly initialization: {e}")
        results["initialization"]["error"] = str(e)
        results["initialization"]["traceback"] = traceback.format_exc()
        return results


def test_f1_metrics_computation(fly_path):
    """
    Test F1-specific metrics computation.

    Returns:
        dict: F1 metrics test results
    """
    print(f"\nğŸ§ª Testing F1 Metrics Computation")
    print(f"{'='*60}")

    results = {
        "f1_metrics": {"success": False, "metrics": {}, "error": None},
        "ballpushing_metrics": {"success": False, "metrics": {}, "error": None},
        "compatibility": {"success": False, "details": {}},
    }

    try:
        # Load fly
        f1_fly = Fly(fly_path, as_individual=True)

        if (
            f1_fly.tracking_data is None
            or not hasattr(f1_fly.tracking_data, "valid_data")
            or not f1_fly.tracking_data.valid_data
        ):
            results["f1_metrics"]["error"] = "Invalid tracking data"
            return results

        print(f"ğŸ”¬ Computing F1-specific metrics...")

        # Test F1 metrics
        try:
            start_time = time.time()
            f1_metrics = F1Metrics(f1_fly.tracking_data)
            f1_compute_time = time.time() - start_time

            print(f"âœ… F1 metrics computed in {f1_compute_time:.3f}s")
            results["f1_metrics"]["success"] = True
            results["f1_metrics"]["compute_time"] = f1_compute_time
            results["f1_metrics"]["metrics"] = convert_to_serializable(f1_metrics.metrics)

            # Analyze F1 metrics
            print(f"ğŸ“Š F1 Metrics Analysis:")
            for metric_name, value in f1_metrics.metrics.items():
                if isinstance(value, (pd.DataFrame, pd.Series)):
                    print(f"   {metric_name}: DataFrame/Series with shape {value.shape}")
                elif isinstance(value, dict):
                    print(f"   {metric_name}: Dict with {len(value)} keys")
                elif isinstance(value, (list, tuple)):
                    print(f"   {metric_name}: List/Tuple with {len(value)} items")
                else:
                    print(f"   {metric_name}: {type(value).__name__} = {value}")

        except Exception as e:
            print(f"âŒ F1 metrics computation failed: {e}")
            results["f1_metrics"]["error"] = str(e)
            results["f1_metrics"]["traceback"] = traceback.format_exc()

        # Test standard ballpushing metrics for comparison
        try:
            print(f"\nğŸ”¬ Computing standard ballpushing metrics for comparison...")
            start_time = time.time()
            bp_metrics = BallPushingMetrics(f1_fly.tracking_data)
            bp_compute_time = time.time() - start_time

            print(f"âœ… Ballpushing metrics computed in {bp_compute_time:.3f}s")
            results["ballpushing_metrics"]["success"] = True
            results["ballpushing_metrics"]["compute_time"] = bp_compute_time

            # Extract sample metrics for comparison
            sample_metrics = {}
            for key, metrics_dict in bp_metrics.metrics.items():
                if isinstance(metrics_dict, dict):
                    # Take a few key metrics
                    for metric in ["nb_events", "max_event", "chamber_ratio", "has_finished"]:
                        if metric in metrics_dict:
                            sample_key = f"{key}_{metric}"
                            sample_metrics[sample_key] = convert_to_serializable(metrics_dict[metric])

            results["ballpushing_metrics"]["sample_metrics"] = sample_metrics

            print(f"ğŸ“Š Sample Ballpushing Metrics:")
            for key, value in sample_metrics.items():
                print(f"   {key}: {value}")

        except Exception as e:
            print(f"âŒ Ballpushing metrics computation failed: {e}")
            results["ballpushing_metrics"]["error"] = str(e)

        # Test compatibility between F1 and standard metrics
        if results["f1_metrics"]["success"] and results["ballpushing_metrics"]["success"]:
            print(f"\nğŸ”— Testing F1/Ballpushing compatibility...")
            compatibility = results["compatibility"]["details"]

            # Compare computation times
            f1_time = results["f1_metrics"]["compute_time"]
            bp_time = results["ballpushing_metrics"]["compute_time"]
            compatibility["relative_compute_time"] = f1_time / bp_time if bp_time > 0 else None

            print(f"â±ï¸  Relative computation time: F1={f1_time:.3f}s vs BP={bp_time:.3f}s")
            print(
                f"   Ratio: {compatibility['relative_compute_time']:.2f}x"
                if compatibility["relative_compute_time"]
                else ""
            )

            # Check for overlapping data requirements
            f1_requires_dual_balls = "training_ball_distances" in results["f1_metrics"]["metrics"]
            bp_works_with_dual = results["ballpushing_metrics"]["success"]

            compatibility["f1_dual_ball_support"] = f1_requires_dual_balls
            compatibility["bp_dual_ball_tolerance"] = bp_works_with_dual

            if f1_requires_dual_balls and bp_works_with_dual:
                results["compatibility"]["success"] = True
                print(f"âœ… F1 and standard ballpushing metrics are compatible")
            else:
                print(f"âš ï¸  Potential compatibility issues detected")

        return results

    except Exception as e:
        print(f"âŒ Error during metrics testing: {e}")
        results["f1_metrics"]["error"] = str(e)
        return results


def test_f1_specific_features(fly_path):
    """
    Test F1-specific features and functionality.

    Returns:
        dict: F1 feature test results
    """
    print(f"\nğŸ§ª Testing F1-Specific Features")
    print(f"{'='*60}")

    results = {
        "checkpoint_analysis": {"success": False, "checkpoints": {}},
        "ball_classification": {"success": False, "classification": {}},
        "direction_matching": {"success": False, "direction_data": {}},
        "adjusted_timing": {"success": False, "timing_data": {}},
    }

    try:
        # Load fly and compute F1 metrics
        f1_fly = Fly(fly_path, as_individual=True)
        f1_metrics = F1Metrics(f1_fly.tracking_data)

        # Test checkpoint analysis
        print(f"ğŸ¯ Testing checkpoint analysis...")
        checkpoints = f1_metrics.F1_checkpoints
        if checkpoints:
            results["checkpoint_analysis"]["success"] = True
            results["checkpoint_analysis"]["checkpoints"] = convert_to_serializable(checkpoints)

            print(f"âœ… F1 checkpoints computed:")
            for distance, time_val in checkpoints.items():
                status = f"{time_val:.3f}s" if time_val is not None else "Not reached"
                print(f"   {distance}px: {status}")
        else:
            print(f"âš ï¸  No checkpoints computed")

        # Test ball classification (training vs test)
        print(f"\nğŸ€ Testing ball classification...")
        training_data = f1_metrics.training_ball_distances
        test_data = f1_metrics.test_ball_distances

        classification = results["ball_classification"]["classification"]

        if training_data is not None:
            classification["training_ball_detected"] = True
            classification["training_data_length"] = len(training_data)
            classification["training_initial_pos"] = (
                float(training_data["x_centre"].iloc[0]),
                float(training_data["y_centre"].iloc[0]),
            )
            print(f"âœ… Training ball detected: {len(training_data)} frames")
        else:
            classification["training_ball_detected"] = False
            print(f"âš ï¸  No training ball detected")

        if test_data is not None:
            classification["test_ball_detected"] = True
            classification["test_data_length"] = len(test_data)
            classification["test_initial_pos"] = (
                float(test_data["x_centre"].iloc[0]),
                float(test_data["y_centre"].iloc[0]),
            )
            print(f"âœ… Test ball detected: {len(test_data)} frames")
        else:
            classification["test_ball_detected"] = False
            print(f"âš ï¸  No test ball detected")

        if classification["training_ball_detected"] and classification["test_ball_detected"]:
            results["ball_classification"]["success"] = True

            # Calculate separation between balls
            train_pos = classification["training_initial_pos"]
            test_pos = classification["test_initial_pos"]
            separation = np.sqrt((train_pos[0] - test_pos[0]) ** 2 + (train_pos[1] - test_pos[1]) ** 2)
            classification["ball_separation_px"] = float(separation)

            print(f"ğŸ“ Ball separation: {separation:.1f} pixels")

        # Test direction matching
        print(f"\nğŸ§­ Testing direction matching...")
        direction_match = f1_metrics.direction_match

        if direction_match is not None:
            results["direction_matching"]["success"] = True
            results["direction_matching"]["direction_data"]["match_result"] = direction_match
            print(f"âœ… Direction matching result: {direction_match}")
        else:
            print(f"âš ï¸  Direction matching not available")

        # Test adjusted timing
        print(f"\nâ° Testing adjusted timing...")
        adjusted_time = f1_metrics.metrics.get("adjusted_time")

        if adjusted_time is not None:
            results["adjusted_timing"]["success"] = True
            timing_data = results["adjusted_timing"]["timing_data"]

            if hasattr(adjusted_time, "shape"):
                timing_data["data_type"] = "Series/Array"
                timing_data["length"] = len(adjusted_time)
                timing_data["range"] = (float(adjusted_time.min()), float(adjusted_time.max()))
            else:
                timing_data["data_type"] = type(adjusted_time).__name__
                timing_data["value"] = convert_to_serializable(adjusted_time)

            print(f"âœ… Adjusted timing computed: {timing_data}")
        else:
            print(f"âš ï¸  Adjusted timing not available")

        return results

    except Exception as e:
        print(f"âŒ Error during F1 features testing: {e}")
        for key in results:
            results[key]["error"] = str(e)
        return results


def run_comprehensive_f1_test(fly_path):
    """
    Run a comprehensive test of F1 experiment processing.

    Returns:
        dict: Complete test results
    """
    print(f"\n{'='*80}")
    print(f"ğŸ§ª COMPREHENSIVE F1 EXPERIMENT TEST")
    print(f"{'='*80}")
    print(f"ğŸ“ Test fly: {fly_path}")
    print(f"ğŸ• Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Initialize comprehensive results
    comprehensive_results = {
        "test_info": {
            "fly_path": str(fly_path),
            "test_start_time": datetime.now().isoformat(),
            "test_duration": None,
        },
        "validation": {},
        "initialization": {},
        "metrics_computation": {},
        "f1_features": {},
        "overall_success": False,
        "summary": {},
    }

    test_start_time = time.time()

    try:
        # Step 1: Directory validation
        print(f"\n" + "=" * 60)
        print(f"STEP 1: F1 Directory Validation")
        print(f"=" * 60)

        validation_results = validate_f1_fly_directory(fly_path)
        comprehensive_results["validation"] = validation_results

        if not validation_results["valid"]:
            print(f"âŒ Directory validation failed - stopping test")
            return comprehensive_results

        # Step 2: Fly initialization
        print(f"\n" + "=" * 60)
        print(f"STEP 2: F1 Fly Initialization")
        print(f"=" * 60)

        init_results = test_f1_fly_initialization(fly_path)
        comprehensive_results["initialization"] = init_results

        if not init_results["initialization"]["success"]:
            print(f"âŒ Fly initialization failed - stopping test")
            return comprehensive_results

        # Step 3: Metrics computation
        print(f"\n" + "=" * 60)
        print(f"STEP 3: F1 Metrics Computation")
        print(f"=" * 60)

        metrics_results = test_f1_metrics_computation(fly_path)
        comprehensive_results["metrics_computation"] = metrics_results

        # Step 4: F1-specific features (continue even if metrics fail)
        print(f"\n" + "=" * 60)
        print(f"STEP 4: F1-Specific Features")
        print(f"=" * 60)

        if metrics_results["f1_metrics"]["success"]:
            features_results = test_f1_specific_features(fly_path)
            comprehensive_results["f1_features"] = features_results
        else:
            print(f"âš ï¸  Skipping F1 features test due to metrics computation failure")
            comprehensive_results["f1_features"] = {"skipped": True, "reason": "metrics_failed"}

        # Calculate test duration
        test_duration = time.time() - test_start_time
        comprehensive_results["test_info"]["test_duration"] = test_duration
        comprehensive_results["test_info"]["test_end_time"] = datetime.now().isoformat()

        # Determine overall success
        overall_success = (
            validation_results["valid"]
            and init_results["initialization"]["success"]
            and init_results["tracking_data"]["success"]
            and metrics_results["f1_metrics"]["success"]
        )

        comprehensive_results["overall_success"] = overall_success

        # Generate summary
        summary = comprehensive_results["summary"]
        summary["validation_passed"] = validation_results["valid"]
        summary["initialization_passed"] = init_results["initialization"]["success"]
        summary["f1_metrics_passed"] = metrics_results["f1_metrics"]["success"]
        summary["ballpushing_metrics_passed"] = metrics_results["ballpushing_metrics"]["success"]
        summary["f1_compatibility"] = metrics_results.get("compatibility", {}).get("success", False)

        if "f1_features" in comprehensive_results and not comprehensive_results["f1_features"].get("skipped"):
            features = comprehensive_results["f1_features"]
            summary["checkpoint_analysis_passed"] = features.get("checkpoint_analysis", {}).get("success", False)
            summary["ball_classification_passed"] = features.get("ball_classification", {}).get("success", False)

        summary["test_duration_seconds"] = test_duration
        summary["overall_success"] = overall_success

        # Print final summary
        print(f"\n" + "=" * 80)
        print(f"ğŸ“Š COMPREHENSIVE TEST SUMMARY")
        print(f"=" * 80)
        print(f"ğŸ• Total duration: {test_duration:.3f} seconds")
        print(f"ğŸ“ Test fly: {Path(fly_path).name}")

        print(f"\nğŸ“‹ Test Results:")
        for test_name, passed in summary.items():
            if test_name.endswith("_passed"):
                emoji = "âœ…" if passed else "âŒ"
                display_name = test_name.replace("_passed", "").replace("_", " ").title()
                print(f"   {emoji} {display_name}")

        overall_emoji = "âœ…" if overall_success else "âŒ"
        print(f"\n{overall_emoji} OVERALL RESULT: {'PASSED' if overall_success else 'FAILED'}")

        if overall_success:
            print(f"\nğŸ‰ F1 experiment processing is working correctly!")
        else:
            print(f"\nâš ï¸  F1 experiment processing needs attention.")
            print(f"   Check individual test results for details.")

        return comprehensive_results

    except Exception as e:
        print(f"âŒ Unexpected error during comprehensive test: {e}")
        comprehensive_results["test_info"]["unexpected_error"] = str(e)
        comprehensive_results["test_info"]["traceback"] = traceback.format_exc()
        return comprehensive_results


def save_test_results(results, output_dir=None):
    """Save test results to JSON file."""
    if output_dir is None:
        output_dir = Path(__file__).parent / "outputs"

    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)

    # Generate filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    fly_name = Path(results["test_info"]["fly_path"]).name
    filename = f"f1_test_results_{fly_name}_{timestamp}.json"

    output_file = output_dir / filename

    with open(output_file, "w") as f:
        json.dump(results, f, indent=2, default=str)

    print(f"\nğŸ’¾ Test results saved to: {output_file}")
    return output_file


def main():
    """Main function for F1 metrics testing."""
    parser = argparse.ArgumentParser(
        description="Test F1 experiment data processing and metrics computation.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Test a single F1 fly with comprehensive analysis
    python f1_metrics_test.py --mode fly --path /path/to/f1/fly --test comprehensive

    # Test F1-specific metrics only
    python f1_metrics_test.py --mode fly --path /path/to/f1/fly --test f1_features

    # Quick validation test
    python f1_metrics_test.py --mode fly --path /path/to/f1/fly --test validation
        """,
    )

    parser.add_argument(
        "--mode",
        choices=["fly", "experiment"],
        required=True,
        help="Specify whether to process a single fly or an experiment.",
    )

    parser.add_argument("--path", required=True, help="Path to the F1 fly or experiment data.")

    parser.add_argument(
        "--test",
        choices=["validation", "initialization", "metrics", "f1_features", "comprehensive"],
        default="comprehensive",
        help="Type of test to run (default: comprehensive).",
    )

    parser.add_argument("--output-dir", help="Directory to save test results (default: tests/integration/outputs).")

    parser.add_argument("--save-results", action="store_true", help="Save detailed test results to JSON file.")

    args = parser.parse_args()

    # Convert path to Path object
    data_path = Path(args.path)

    print(f"ğŸ§ª F1 EXPERIMENT TESTING SUITE")
    print(f"{'='*80}")
    print(f"Mode: {args.mode}")
    print(f"Path: {data_path}")
    print(f"Test type: {args.test}")
    print(f"{'='*80}")

    # Initialize results variable
    results = None

    # Run tests based on mode and test type
    if args.mode == "fly":
        if args.test == "validation":
            results = {"validation": validate_f1_fly_directory(data_path)}
        elif args.test == "initialization":
            results = {"initialization": test_f1_fly_initialization(data_path)}
        elif args.test == "metrics":
            results = {"metrics": test_f1_metrics_computation(data_path)}
        elif args.test == "f1_features":
            results = {"f1_features": test_f1_specific_features(data_path)}
        else:  # comprehensive
            results = run_comprehensive_f1_test(data_path)

    elif args.mode == "experiment":
        print(f"âš ï¸  Experiment mode not yet implemented - testing first fly only")
        # For now, test the first fly in the experiment
        first_fly = None
        for item in data_path.iterdir():
            if item.is_dir():
                first_fly = item
                break

        if first_fly:
            print(f"ğŸ“ Testing first fly found: {first_fly}")
            results = run_comprehensive_f1_test(first_fly)
        else:
            print(f"âŒ No fly directories found in experiment path")
            return 1

    # Save results if requested
    if args.save_results and results:
        save_test_results(results, args.output_dir)

    # Return appropriate exit code
    if results and results.get("overall_success", False):
        print(f"\nâœ… All tests completed successfully!")
        return 0
    else:
        print(f"\nâŒ Some tests failed - check results for details")
        return 1


if __name__ == "__main__":
    exit(main())
