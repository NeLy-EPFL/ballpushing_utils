#!/usr/bin/env python3
"""
Script to analyze and summarize TNT screen SLEAP analysis results.

This script explores the tnt_screen_analysis output directory and analyzes
the CSV files generated by the SLEAP video path consistency checker.

Usage:
    python analyze_tnt_screen_results.py [--output-dir PATH]
"""

import argparse
import pandas as pd
from pathlib import Path
import sys
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import numpy as np


def find_csv_files(output_dir: Path) -> Dict[str, List[Path]]:
    """
    Find all CSV files in the output directory, organized by file type.

    Parameters
    ----------
    output_dir : Path
        Output directory to search

    Returns
    -------
    Dict[str, List[Path]]
        Dictionary mapping file types to lists of CSV files
    """
    csv_files = {"full_body": [], "ball": [], "fly": []}

    if not output_dir.exists():
        print(f"❌ Output directory does not exist: {output_dir}")
        return csv_files

    # Find all CSV files
    all_csvs = list(output_dir.glob("sleap_mismatches_*.csv"))

    for csv_file in all_csvs:
        filename = csv_file.name
        if "full_body" in filename:
            csv_files["full_body"].append(csv_file)
        elif "ball" in filename:
            csv_files["ball"].append(csv_file)
        elif "fly" in filename:
            csv_files["fly"].append(csv_file)

    return csv_files


def load_and_combine_csvs(csv_files: List[Path]) -> pd.DataFrame:
    """
    Load and combine multiple CSV files into a single DataFrame.

    Parameters
    ----------
    csv_files : List[Path]
        List of CSV files to load

    Returns
    -------
    pd.DataFrame
        Combined DataFrame
    """
    if not csv_files:
        return pd.DataFrame()

    dfs = []
    for csv_file in csv_files:
        try:
            df = pd.read_csv(csv_file)
            df["source_csv"] = csv_file.name
            dfs.append(df)
        except Exception as e:
            print(f"⚠️  Error loading {csv_file}: {e}")

    if dfs:
        return pd.concat(dfs, ignore_index=True)
    else:
        return pd.DataFrame()


def analyze_error_types(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Analyze error types in the data.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing mismatch data

    Returns
    -------
    Dict[str, Any]
        Analysis results
    """
    if df.empty:
        return {}

    error_counts = df["error_type"].value_counts()

    analysis = {
        "total_files": len(df),
        "error_types": error_counts.to_dict(),
        "most_common_error": error_counts.index[0] if not error_counts.empty else None,
        "error_type_percentages": (error_counts / len(df) * 100).round(2).to_dict(),
    }

    return analysis


def analyze_experiment_patterns(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Analyze patterns across experiments and sessions.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing mismatch data

    Returns
    -------
    Dict[str, Any]
        Analysis results
    """
    if df.empty:
        return {}

    # Filter out empty experiment dates
    df_clean = df[df["experiment_date"].notna() & (df["experiment_date"] != "")]

    if df_clean.empty:
        return {"message": "No valid experiment date information found"}

    experiment_counts = df_clean["experiment_date"].value_counts()
    session_counts = df_clean["experiment_session"].value_counts()

    analysis = {
        "total_experiments_affected": len(experiment_counts),
        "total_sessions_affected": len(session_counts),
        "experiments_with_issues": experiment_counts.to_dict(),
        "sessions_with_issues": session_counts.to_dict(),
        "most_problematic_experiment": experiment_counts.index[0] if not experiment_counts.empty else None,
        "most_problematic_session": session_counts.index[0] if not session_counts.empty else None,
    }

    return analysis


def analyze_arena_corridor_patterns(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Analyze arena/corridor mismatch patterns.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing mismatch data

    Returns
    -------
    Dict[str, Any]
        Analysis results
    """
    if df.empty:
        return {}

    # Filter for arena/corridor mismatches
    mismatch_df = df[df["error_type"] == "arena_corridor_mismatch"].copy()

    if mismatch_df.empty:
        return {"message": "No arena/corridor mismatches found"}

    # Create arena/corridor combination strings for analysis
    mismatch_df["h5_location"] = mismatch_df["h5_arena"].astype(str) + "/" + mismatch_df["h5_corridor"].astype(str)
    mismatch_df["video_location"] = (
        mismatch_df["video_arena"].astype(str) + "/" + mismatch_df["video_corridor"].astype(str)
    )
    mismatch_df["mismatch_pattern"] = mismatch_df["h5_location"] + " → " + mismatch_df["video_location"]

    pattern_counts = mismatch_df["mismatch_pattern"].value_counts()
    h5_location_counts = mismatch_df["h5_location"].value_counts()
    video_location_counts = mismatch_df["video_location"].value_counts()

    analysis = {
        "total_arena_corridor_mismatches": len(mismatch_df),
        "unique_mismatch_patterns": len(pattern_counts),
        "most_common_patterns": pattern_counts.head(10).to_dict(),
        "most_problematic_h5_locations": h5_location_counts.head(5).to_dict(),
        "most_common_video_targets": video_location_counts.head(5).to_dict(),
    }

    return analysis


def create_summary_visualizations(results: Dict[str, Dict], output_dir: Path) -> None:
    """
    Create summary visualizations of the analysis results.

    Parameters
    ----------
    results : Dict[str, Dict]
        Analysis results for each file type
    output_dir : Path
        Directory to save visualizations
    """
    plt.style.use("seaborn-v0_8")
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle("TNT Screen SLEAP Analysis Summary", fontsize=16, fontweight="bold")

    # 1. File type comparison
    ax1 = axes[0, 0]
    file_types = []
    total_issues = []

    for file_type, data in results.items():
        if "total_issues_found" in data:
            file_types.append(file_type.replace("_", " ").title())
            total_issues.append(data["total_issues_found"])

    if file_types:
        bars = ax1.bar(file_types, total_issues, color=["#ff7f0e", "#2ca02c", "#d62728"])
        ax1.set_title("Issues by File Type")
        ax1.set_ylabel("Number of Files with Issues")

        # Add value labels on bars
        for bar, value in zip(bars, total_issues):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width() / 2.0, height + 0.1, f"{value}", ha="center", va="bottom")

    # 2. Error type distribution (combined)
    ax2 = axes[0, 1]
    all_error_types = {}

    for file_type, data in results.items():
        if "error_analysis" in data and data["error_analysis"] and "error_types" in data["error_analysis"]:
            for error_type, count in data["error_analysis"]["error_types"].items():
                all_error_types[error_type] = all_error_types.get(error_type, 0) + count

    if all_error_types:
        error_types = list(all_error_types.keys())
        error_counts = list(all_error_types.values())

        wedges, texts, autotexts = ax2.pie(error_counts, labels=error_types, autopct="%1.1f%%", startangle=90)
        ax2.set_title("Distribution of Error Types")

    # 3. Experiments with issues
    ax3 = axes[1, 0]
    all_experiments = {}

    for file_type, data in results.items():
        if (
            "experiment_analysis" in data
            and data["experiment_analysis"]
            and "experiments_with_issues" in data["experiment_analysis"]
        ):
            for exp, count in data["experiment_analysis"]["experiments_with_issues"].items():
                all_experiments[exp] = all_experiments.get(exp, 0) + count

    if all_experiments:
        # Show top 10 experiments
        exp_items = sorted(all_experiments.items(), key=lambda x: x[1], reverse=True)[:10]
        experiments, counts = zip(*exp_items)

        bars = ax3.barh(range(len(experiments)), counts)
        ax3.set_yticks(range(len(experiments)))
        ax3.set_yticklabels(experiments)
        ax3.set_xlabel("Number of Issues")
        ax3.set_title("Top 10 Experiments with Issues")
        ax3.invert_yaxis()

    # 4. Summary statistics text
    ax4 = axes[1, 1]
    ax4.axis("off")

    # Create summary text
    summary_text = "SUMMARY STATISTICS\n\n"

    total_estimated_files = sum(data.get("estimated_total_files", 0) for data in results.values())
    total_issues_found = sum(data.get("total_issues_found", 0) for data in results.values())

    summary_text += f"Est. total files: {total_estimated_files:,}\n"
    summary_text += f"Files with issues: {total_issues_found:,}\n"

    if total_estimated_files > 0:
        issue_rate = (total_issues_found / total_estimated_files) * 100
        summary_text += f"Est. issue rate: {issue_rate:.2f}%\n\n"

    summary_text += "Issues by file type:\n"
    for file_type, data in results.items():
        if "total_issues_found" in data:
            issues = data["total_issues_found"]
            estimated = data.get("estimated_total_files", 0)
            summary_text += f"• {file_type}: {issues}/{estimated}\n"

    ax4.text(
        0.05,
        0.95,
        summary_text,
        transform=ax4.transAxes,
        fontsize=11,
        verticalalignment="top",
        fontfamily="monospace",
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"),
    )

    plt.tight_layout()

    # Save the plot
    plot_file = output_dir / f"tnt_screen_analysis_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
    plt.savefig(plot_file, dpi=300, bbox_inches="tight")
    print(f"📊 Summary visualization saved to: {plot_file}")

    plt.show()


def print_detailed_summary(results: Dict[str, Dict]) -> None:
    """
    Print a detailed summary of the analysis results.

    Parameters
    ----------
    results : Dict[str, Dict]
        Analysis results for each file type
    """
    print(f"\n{'='*80}")
    print(f"🔍 TNT SCREEN SLEAP ANALYSIS SUMMARY")
    print(f"{'='*80}")

    # Overall statistics
    total_estimated_files = sum(data.get("estimated_total_files", 0) for data in results.values())
    total_issues_found = sum(data.get("total_issues_found", 0) for data in results.values())

    print(f"📊 OVERALL STATISTICS:")
    print(f"   Estimated total files processed: {total_estimated_files:,}")
    print(f"   Total files with issues: {total_issues_found:,}")

    if total_estimated_files > 0:
        issue_rate = (total_issues_found / total_estimated_files) * 100
        print(f"   Estimated issue rate: {issue_rate:.2f}%")

    print(f"   📝 Note: CSV files contain only problematic files, not all files checked")
    print()

    # File type breakdown
    for file_type, data in results.items():
        print(f"\n📁 {file_type.upper()} FILES:")

        if "note" in data:
            print(f"   📝 {data['note']}")

        if "total_issues_found" in data:
            print(f"   Issues found: {data['total_issues_found']:,}")

        if "estimated_total_files" in data:
            print(f"   Estimated total files: {data['estimated_total_files']:,}")

            if data["estimated_total_files"] > 0:
                rate = (data["total_issues_found"] / data["estimated_total_files"]) * 100
                print(f"   Estimated issue rate: {rate:.2f}%")

        if "error_analysis" in data and data["error_analysis"]:
            error_data = data["error_analysis"]

            if "error_types" in error_data:
                print(f"   Error breakdown:")
                for error_type, count in error_data["error_types"].items():
                    percentage = error_data["error_type_percentages"].get(error_type, 0)
                    print(f"     • {error_type.replace('_', ' ').title()}: {count} ({percentage:.1f}%)")
        else:
            if data.get("total_issues_found", 0) == 0:
                print(f"   ✅ No issues found!")

        if "experiment_analysis" in data and data["experiment_analysis"]:
            exp_data = data["experiment_analysis"]
            if "total_experiments_affected" in exp_data:
                print(f"   Experiments affected: {exp_data['total_experiments_affected']}")
                print(f"   Sessions affected: {exp_data['total_sessions_affected']}")

                if "most_problematic_experiment" in exp_data and exp_data["most_problematic_experiment"]:
                    exp_name = exp_data["most_problematic_experiment"]
                    exp_count = exp_data["experiments_with_issues"][exp_name]
                    print(f"   Most problematic experiment: {exp_name} ({exp_count} issues)")

        if "arena_analysis" in data and data["arena_analysis"]:
            arena_data = data["arena_analysis"]
            if "total_arena_corridor_mismatches" in arena_data:
                print(f"   Arena/corridor mismatches: {arena_data['total_arena_corridor_mismatches']}")
                print(f"   Unique mismatch patterns: {arena_data['unique_mismatch_patterns']}")

                if "most_common_patterns" in arena_data and arena_data["most_common_patterns"]:
                    print(f"   Most common mismatch patterns:")
                    for pattern, count in list(arena_data["most_common_patterns"].items())[:3]:
                        print(f"     • {pattern}: {count} occurrences")


def analyze_single_file_type(csv_files: List[Path], file_type: str) -> Dict[str, Any]:
    """
    Analyze a single file type's CSV data.

    Parameters
    ----------
    csv_files : List[Path]
        CSV files for this file type
    file_type : str
        Type of files being analyzed

    Returns
    -------
    Dict[str, Any]
        Analysis results
    """
    if not csv_files:
        return {"message": f"No CSV files found for {file_type}"}

    print(f"📈 Analyzing {file_type} files...")
    print(f"   CSV files found: {len(csv_files)}")

    # Load and combine data
    df = load_and_combine_csvs(csv_files)

    if df.empty:
        return {"message": f"No data found in {file_type} CSV files"}

    # Since CSV files only contain problematic files, the total issues = number of rows
    # We need to estimate total files checked from the number of experiment directories
    # Each experiment typically has 6 arenas x 6 corridors = 36 files per experiment
    experiments = df["experiment_date"].nunique() if "experiment_date" in df.columns else 1
    estimated_total_files = experiments * 36  # Rough estimate

    # Or we could try to extract from log files or make a note that this is only problematic files

    # Perform analyses
    error_analysis = analyze_error_types(df)
    experiment_analysis = analyze_experiment_patterns(df)
    arena_analysis = analyze_arena_corridor_patterns(df)

    results = {
        "csv_files_analyzed": len(csv_files),
        "total_issues_found": len(df),
        "estimated_total_files": estimated_total_files,
        "note": "CSV files contain only problematic files, not all files checked",
        "error_analysis": error_analysis,
        "experiment_analysis": experiment_analysis,
        "arena_analysis": arena_analysis,
    }

    return results


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Analyze TNT screen SLEAP analysis results")
    parser.add_argument(
        "--output-dir",
        "-o",
        type=str,
        default=None,
        help="Output directory to analyze (default: outputs/tnt_screen_analysis)",
    )
    parser.add_argument("--no-plot", "-n", action="store_true", help="Skip creating visualization plots")

    args = parser.parse_args()

    # Determine output directory
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        # Default to tnt_screen_analysis directory
        script_dir = Path(__file__).parent
        output_dir = script_dir.parent / "outputs" / "tnt_screen_analysis"

    print(f"🔍 Analyzing results in: {output_dir}")

    # Find CSV files
    csv_files = find_csv_files(output_dir)

    if not any(csv_files.values()):
        print(f"❌ No CSV files found in {output_dir}")
        print("Make sure the TNT screen analysis has been run and generated CSV files.")
        sys.exit(1)

    print(f"📁 CSV files found:")
    for file_type, files in csv_files.items():
        print(f"   {file_type}: {len(files)} files")

    # Analyze each file type
    results = {}
    for file_type, files in csv_files.items():
        if files:
            results[file_type] = analyze_single_file_type(files, file_type)

    # Print detailed summary
    print_detailed_summary(results)

    # Create visualizations if requested
    if not args.no_plot and results:
        try:
            create_summary_visualizations(results, output_dir)
        except Exception as e:
            print(f"⚠️  Could not create visualizations: {e}")
            print("Install matplotlib and seaborn to enable plots: pip install matplotlib seaborn")

    print(f"\n✅ Analysis complete!")


if __name__ == "__main__":
    main()
